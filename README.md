# Algorithms Machine Learning
Машинное обучение связано с разработкой алгоритмов, которые можно обучать (настраивать) под решение конкретных задач.

Здесь собраны основные алгоритмы машинного обучения, и их применение в задачах классификации, регрессии, кластеризации итд.


## Gradient algorithm
Метод градиентного спуска основан на алгоритме направленного поиска, или поиска точки экстремума, с помощью градиента.
В задачах машинного обучения, при использовании метода градиентного спуска чаще всего необходимо минимизировать функцию потерь, для чего используется антиградиент. Функция потерь выбирается под конкретную задачу.


В общем случае, формула поиска минимума выглядит так:
### $x_{n+1} = x_{n} - \lambda_{n} * \frac{df(x)}{dx}$
Стохастический градиентный спуск (SGD) - основан на использовании псевдоградиента, когда расчет производится не по всей выборке, а по ее части (один образ или K образов).

Усредненный стохастический градиент (SAG) - в основе реализации лежит алгоритм SGD, но градиент рассчитывается по всей выборке и усредняется.

При работе с градиентным алгоритмом используются оптимизаторы (импульс Нестерова, RMSProp, Adam..) и регуляризаторы (L1, L2).

## Dicision tree
Метод решающих деревьев относится к логическим методам анализа. Для построения дерева используются логические выводы на основе серии конъюнкций бинарных признаков.

Наилучшее разбиение дерева находится путем минимизации impurity (информативности) каждого предиката и общей оценке разбиения information gain (информационный выигрыш). В качестве impurity используюется энтропия, критерий Джини, misclassification error итд.

Для достижения наилучшей модели решающие деревья используют в ансамблях, то есть наборе из многих отдельных моделей. Такой алгоритм называют Random Forest, а предсказание формируется путем голосования всех моделей.

## Linear algorithm
Модели, которые представляют собой линейную комбинацию признаков с некоторыми настраиваемыми параметрами (весами), называются линейными.

В общем случае, при линейной зависимости, решение задачи с помощью линейного алгоритма сводится к построению разделяющей линии или гиперплоскости.
При нелинейной зависимости, чтобы использовать линейный алгоритм необходимо расширить признаковое пространство нелинейными преобразованиями. Такая задача называется аппроксимацией (восстановлением) данных.

## Logistic regression
Алгоритм логистической регрессии сводится к вероятностному взгляду на задачи машинного обучения.

Для решения поставленных задач с помощью такого алгоритма используется метод максимального правдоподобия:
### $\omega = argmax P (y\vert x, \omega)$
Здесь реализованы наивный байесовский алгоритм, гауссовский байесовский классификатор и линейный дискриминант Фишера.

## Clustering
Задача кластеризации работает с неразмеченными данными, которые необходимо распределить (классифицировать) по группам.

Целями кластеризации могут быть сокращение объема выборки, упрощение представления объектов и их обработки в пределах группы, выделение нетипичным объектов (выбросов).

Для задачи кластеризации используют алгоритм Ллойда, DBSCAN, алгоритм агломеративной иерархической кластеризации итд.

## K Nearest Neighbors (KNN)
Метод К-ближайших соседей основан на поиске соседей с минимальным расстоянием до классифицируемого образа. Предсказание формируется по результатам "голосования" К ближайших соседей.

Для поиска расстояний используются различные метрики, например, метрика Минковского или Евклидова. Для учета весов разноудаленных соседей объекта используются парзеновские окна с различными функциями ядра.

## Support Vector Machine (SVM)
Метод опорных векторов схож с линейным алгоритмом: в данном методе также строится разделяющая плоскость. Она ориентируется только на распределение обучающей выборки,  делая минимальные предположения о распределении классов и, тем самым, приводит к лучшей обобщающей способности алгоритма классификации.  С точки зрения SVM, оптимальная разделяющая гиперплоскость – это та, которая образует наиболее широкую полосу между объектами двух классов. При этом сама разделяющая гиперплоскость будет точно проходить посередине этой полосы.

## Principal Component Analysis (PCA)
Метод главных компонент является одним из способов борьбы с переобучением. С помощью этого метода сокращается признаковое пространство, оставляя наиболее важные характеристики, тем самым упрощая модель.

Для выявления таких характеристик вычисляется матрица Грама, затем находятся собственные векторы и собственные числа. Если отсортировать собственные векторы по убыванию собственных значений, то на первый вектор будут, в среднем, приходиться наибольшие проекции, на второй – наибольшая информация из оставшейся, и так до n-го вектора. Таким образом, мы получаем систему координат, которая наилучшим образом локализует информацию исходного признакового пространства.
